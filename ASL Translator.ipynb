{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accompanied-spectacular",
   "metadata": {},
   "source": [
    "# American Sign Language detector using _mediapipe_ and _scikit-learn_. \n",
    "\n",
    "## Introduction \n",
    "\n",
    "A while back, I discovered [mediapipe](https://google.github.io/mediapipe) and I liked it. It has a very vast variation of detection algorithms for pose, hand, face, iris, etc. \n",
    "\n",
    "I was thinking to myself, what happens if I use this to train a custom model for sign language detection? Although it was done before, I looked at it as a cool project to do in a weekend. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-discussion",
   "metadata": {},
   "source": [
    "## Step 1 - Installing dependencies. \n",
    "\n",
    "In order to get this thing to work, you need to install these packages:\n",
    "\n",
    "* scikit-learn \n",
    "* numpy (I guess if you install scikit, it will be installed automatically)\n",
    "* opencv-python \n",
    "* mediapipe \n",
    "\n",
    "All are available in [pypi](https://pypi.org) and there is nothing really extra-ordinary in this list. \n",
    "\n",
    "## Step 2 - Importing dependencies into the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "manufactured-graduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp \n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-breakfast",
   "metadata": {},
   "source": [
    "## Step 3 - Initializing some needed functionalities from mediapipe\n",
    "\n",
    "Here, I used `mp.solutions.hands`. Depending on what you're going to do, change this line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "usual-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-lithuania",
   "metadata": {},
   "source": [
    "## Step 4 - Just get some input from the camera\n",
    "\n",
    "This is step isn't necessary at all. I only did it because I wanted to make sure how many landmarks I can extract from the hand gestures. So if you now how much data you're dealing with, just ignore this part of the code. \n",
    "\n",
    "P.S : I also copied this code snippet from an old project of mine. You can clearly see I have some parts like `landmark_list = []` left unused on this snippet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "biological-marks",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5, max_num_hands = 1) as hands:\n",
    "    while camera.isOpened():\n",
    "        _, image = camera.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False \n",
    "        results = hands.process(image)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        landmark_list = []\n",
    "        if results.multi_hand_landmarks:\n",
    "            for landmark in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(image, landmark, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "        cv2.imshow(\"Camera No. 1\", cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "found-poster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_coord = len(results.multi_hand_landmarks[0].landmark)\n",
    "num_coord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-flight",
   "metadata": {},
   "source": [
    "## Step 5 - Data gathering \n",
    "\n",
    "Here, I use the same code from step 4 to collect data from the camera. But we obviously need to preprocess the dataset file before doing anything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "confused-motel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-correction",
   "metadata": {},
   "source": [
    "Here I made a list, index 0 is called `class`. You now why? because we need to have a class for each sign. In my case, classes where _Thumbs up, Thumbs down, Rock on, Love_. You can change it to anything you like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "proper-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = ['class']\n",
    "for num in range(1, num_coord + 1):\n",
    "    landmarks += [f'x{num}', f'y{num}', f'z{num}'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aerial-exposure",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks\n",
    "len(landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-dollar",
   "metadata": {},
   "source": [
    "In this line, I just tell my `coords.csv` file that what first line is. This is nothing magicall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "loving-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('coords.csv', mode='w', newline='') as f:\n",
    "    csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow(landmarks)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-silicon",
   "metadata": {},
   "source": [
    "## Step 6 - Adding data to the file \n",
    "\n",
    "Here, I tweaked my camera stuff to add data to the CSV file. Above, you see `class_id` variable. In the body of the loop, we use it as the index 0 of our list. After that, we flatten what we've got from our hand landmarks set. then, we put it in our dataset. \n",
    "\n",
    "For more information, just see `coords.csv` file from this repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-spell",
   "metadata": {},
   "source": [
    "### American Sing Language Guide\n",
    "\n",
    "![ASL](asl.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "structured-present",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_id = \"Nothing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "promising-topic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n"
     ]
    }
   ],
   "source": [
    "camera = cv2.VideoCapture(1)\n",
    "with mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5, max_num_hands = 1) as hands:\n",
    "    while camera.isOpened():\n",
    "        _, image = camera.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False \n",
    "        results = hands.process(image)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        landmark_list = []\n",
    "        if results.multi_hand_landmarks:\n",
    "            for landmark in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(image, landmark, mp_hands.HAND_CONNECTIONS)\n",
    "        \n",
    "        try:\n",
    "            hand = enumerate(results.multi_hand_landmarks[0].landmark)\n",
    "            hand_row = list(np.array([[landmark.x, landmark.y, landmark.z] for _, landmark in hand]).flatten())\n",
    "            hand_row.insert(0, class_id)\n",
    "            #print(hand_row)\n",
    "            with open('coords.csv', mode='a', newline='') as f:\n",
    "                csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                csv_writer.writerow(hand_row)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "        cv2.imshow(\"Camera No. 1\", cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-darwin",
   "metadata": {},
   "source": [
    "## Step 7 - Data Preprocessing \n",
    "\n",
    "Here I don't really do anything magical. This is just preprocessing the data using _pandas_. We open CSV file, we find our X and Y axises and then we put them in `x_train`, `y_train`, `x_test` and `y_test` variables. \n",
    "\n",
    "### Axises \n",
    "\n",
    "* X: It is dedicated to the features we needed. Like coordinations of fingers and finger tips. \n",
    "* Y: It is dedicated to the labels we needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "residential-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bigger-necklace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('coords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "blocked-minnesota",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('class', axis=1)\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "floating-packet",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-begin",
   "metadata": {},
   "source": [
    "## Step 8 - Finding which algorithm is better \n",
    "\n",
    "Here, I just used different algorithms, as you can see. It was necessary to know which learning algorithm is better for this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "retired-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-google",
   "metadata": {},
   "source": [
    "As you can see here, I just put different algorithms here in a bunch of pipelines. For some reason `LogisticRegression` didn't work for me. So if you can solve its problem, I'll appreciate a pull request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "atomic-atlanta",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    #'lr':make_pipeline(StandardScaler(), LogisticRegression()),\n",
    "    'rc':make_pipeline(StandardScaler(), RidgeClassifier()),\n",
    "    'rf':make_pipeline(StandardScaler(), RandomForestClassifier()),\n",
    "    'gb':make_pipeline(StandardScaler(), GradientBoostingClassifier()),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-seafood",
   "metadata": {},
   "source": [
    "Here, we just train our models. This is the time consuming part. Depending on the size of your dataset, it may take a few minutes up to a few hours. \n",
    "\n",
    "On my system (a 2019 MacBook Pro with i7 chip and 16 GB's of RAM) it took almost 5 or 6 minutes. I haven't tested it on [Colab](https://colab.research.google.com) or any other computers. But there most be no significant difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "involved-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_models = {}\n",
    "for algo, pipeline in pipelines.items():\n",
    "    model = pipeline.fit(x_train, y_train)\n",
    "    fit_models[algo] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-colon",
   "metadata": {},
   "source": [
    "Here, we use `sklearn.metrics` to find which model is better. In my case, `RandomForestClassifier` seemed a little bit better, so I went with that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "earlier-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "appreciated-pleasure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rc 0.9608832807570978\n",
      "rf 0.9916403785488959\n",
      "gb 0.9858044164037855\n"
     ]
    }
   ],
   "source": [
    "for algo, model in fit_models.items():\n",
    "    yhat = model.predict(x_test)\n",
    "    print(algo, accuracy_score(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-glasgow",
   "metadata": {},
   "source": [
    "## Final Step - Let the party begin!\n",
    "\n",
    "And I don't have any other explanation for th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "composite-praise",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = fit_models['rf']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "crazy-charm",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = cv2.VideoCapture(1)\n",
    "with mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5, max_num_hands = 1) as hands:\n",
    "    while camera.isOpened():\n",
    "        _, image = camera.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False \n",
    "        results = hands.process(image)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        landmark_list = []\n",
    "        if results.multi_hand_landmarks:\n",
    "            for landmark in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(image, landmark, mp_hands.HAND_CONNECTIONS)\n",
    "        \n",
    "        try:\n",
    "            hand = enumerate(results.multi_hand_landmarks[0].landmark)\n",
    "            hand_row = list(np.array([[landmark.x, landmark.y, landmark.z] for _, landmark in hand]).flatten())\n",
    "            x = pd.DataFrame([hand_row])\n",
    "            prediction = model.predict(x)\n",
    "            prediction = prediction[0]\n",
    "            cv2.putText(image, prediction, (0, 150), cv2.FONT_HERSHEY_COMPLEX, 4, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                \n",
    "        except Exception as e:\n",
    "            pass\n",
    "            \n",
    "        cv2.imshow(\"Camera No. 1\", cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-senegal",
   "metadata": {},
   "source": [
    "## Saving model for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "digital-bandwidth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-blocking",
   "metadata": {},
   "source": [
    "Now, we just pickle the hell out of our freaking model, so we can use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "electric-humanity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.pipeline.Pipeline"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "robust-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(obj=model, file=f, fix_imports=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-senator",
   "metadata": {},
   "source": [
    "Done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
